import spacy
from nltk.corpus import wordnet

# Make sure you have NLTK installed (you can install it with: pip install nltk)

# Load the spaCy model
nlp = spacy.load("en_core_web_sm")

# Initialize NLTK's WordNet
import nltk


# Define a function to extract and find synonyms of keywords
def find_keyword_synonyms(abstract):
    # Process the abstract using spaCy
    doc = nlp(abstract)

    # Extract keywords (nouns and verbs) from the abstract
    keywords = []
    for token in doc:
        if token.pos_ in ("NOUN", "VERB"):
            keywords.append(token.text)

    # Initialize a dictionary to store synonyms
    keyword_synonyms = {}

    # Use NLTK's WordNet to find synonyms
    for keyword in keywords:
        synonyms = set()
        for synset in wordnet.synsets(keyword):
            for lemma in synset.lemmas():
                synonyms.add(lemma.name())

        keyword_synonyms[keyword] = list(synonyms)

    synonym_str = ""
    for synonyms_list in keyword_synonyms.values():
        if synonyms_list:
            for word in synonyms_list:
                synonym_str += word + " " if "_" not in word else ""

    return synonym_str


def main():
    # Example abstract
    abstract = "Synthetic data generation, a cornerstone of Generative Artificial Intelligence, signifies a paradigm " \
               "shift in data science by addressing data scarcity and privacy while enabling unprecedented performance. " \
               "As synthetic data gains prominence, questions arise concerning the accuracy of statistical methods " \
               "when applied to synthetic data compared to raw data. In this article, we introduce the Synthetic Data " \
               "Generation for Analytics framework. This framework employs statistical methods on high-fidelity " \
               "synthetic data generated by advanced models such as tabular diffusion and Generative Pre-trained " \
               "Transformer models. These models, trained on raw data, are further enhanced with insights from " \
               "pertinent studies. A significant discovery within this framework is the generational effect: the " \
               "error of a statistical method on synthetic data initially diminishes with added synthetic data but " \
               "may eventually increase or plateau. This phenomenon, rooted in the complexities of replicating raw " \
               "data distributions, highlights a “reflection point”—–an optimal threshold in the size of synthetic " \
               "data determined by specific error metrics. Through three illustrative case studies—sentiment analysis " \
               "of texts, predictive modeling of structured data, and inference in tabular data—–we demonstrate the " \
               "effectiveness of this framework over traditional ones. We underline its potential to amplify various " \
               "statistical methods, including gradient boosting for prediction and hypothesis testing, thereby " \
               "underscoring the transformative potential of synthetic data generation in data science."

    # Call the function and print the results
    synonym_str = find_keyword_synonyms(abstract)
    print(f"All Synonyms:")
    print(synonym_str)


if __name__ == "__main__":
    main()